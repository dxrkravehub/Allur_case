{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**–≠—Ç–æ –º–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –∏–º–∏—Ç–∞—Ü–∏–∏ –≤ –ß–ë –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π/–í–∏–¥–µ–æ –¥–ª—è —Ö–∞–∫–∞—Ç–æ–Ω–∞\n",
        "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: Yolov8s(–¥–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã), –∫–∞—Å—Ç–æ–º–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–µ—Ñ–µ–∫—Ç–æ—Å–∫–æ–ø–∏–∏ + —Å–≤–æ—è –∂–µ—Å—Ç–∫–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è + Florence-2 —á—Ç–æ–±—ã –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Yolov8s**"
      ],
      "metadata": {
        "id": "UxdWY01BgxfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##–î–∞–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –±—ã–ª–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–∑–¥–∞–Ω–æ –º–Ω–æ–π –∑–∞ 48 —á–∞—Å–∞ –≤ –ø–µ—Ä–∏–æ–¥ —Ö–∞–∫–∞—Ç–æ–Ω–∞\n",
        "\n",
        "***–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –±—ã–ª –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –ø–æ—ç—Ç–æ–º—É —è —Ä–∞–±–æ—Ç–∞–ª —Å–æ —Å–≤–æ–∏–º***"
      ],
      "metadata": {
        "id": "QvxmU_FQnw6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9) # —Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Å—Ä–µ–¥—É –≤ Colab"
      ],
      "metadata": {
        "id": "WmQ2IE5_iRlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PF-7BFsrhOTg",
        "outputId": "c9bf9c3a-c0ad-41ae-e118-855416e979de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: flash-attn in /usr/local/lib/python3.12/dist-packages (2.8.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 1.0.1\n",
            "    Uninstalling huggingface-hub-1.0.1:\n",
            "      Successfully uninstalled huggingface-hub-1.0.1\n",
            "Successfully installed huggingface-hub-0.36.0\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Collecting huggingface-hub\n",
            "  Using cached huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (2025.3.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (4.67.1)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub) (1.3.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub) (8.3.0)\n",
            "Using cached huggingface_hub-1.0.1-py3-none-any.whl (503 kB)\n",
            "Installing collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.36.0\n",
            "    Uninstalling huggingface-hub-0.36.0:\n",
            "      Successfully uninstalled huggingface-hub-0.36.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-1.0.1\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (1.0.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface_hub>=0.21.0->accelerate) (8.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "--- –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ ---\n",
            "–î–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å—Ä–µ–¥—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (Runtime -> Restart runtime).\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤–µ—Ä—Å–∏—è CUDA —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≤–∞—à–µ–π —Å—Ä–µ–¥–µ Colab\n",
        "!pip install flash-attn --no-build-isolation\n",
        "\n",
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade huggingface-hub\n",
        "!pip install accelerate\n",
        "!pip install Pillow\n",
        "\n",
        "# —É—Å—Ç–∞–Ω–æ–≤–∏–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è —Å–∞–º–æ–π –º–æ–¥–µ–ª–∏.\n",
        "# !pip install -e .  # –ï—Å–ª–∏ –≤—ã –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–ª–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π Florence-2\n",
        "!pip install huggingface-hub==0.20.0\n",
        "!pip install transformers==4.49.0\n",
        "!pip install pyngrok\n",
        "!pip install ultralytics\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "#@markdown **–í–≤–µ–¥–∏—Ç–µ —Å–Ω–∏–∑—É —Å–≤–æ–π —Ç–æ–∫–µ–Ω Ngrok –¥–ª—è Gradio**\n",
        "NGROK_TOKEN = ''#@param {type:\"string\"}\n",
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω–∞\n",
        "ngrok.set_auth_token(NGROK_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "FI9lCDe5l01j",
        "outputId": "c48dd35f-5a92-4d8a-b96a-447652cfdf5c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyngrok'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-946409784.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgetpass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# –í–≤–µ–¥–∏—Ç–µ –≤–∞—à —Ç–æ–∫–µ–Ω ngrok, –∫–æ–≥–¥–∞ Colab –ø—Ä–µ–¥–ª–æ–∂–∏—Ç\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"–í–≤–µ–¥–∏—Ç–µ –≤–∞—à —Ç–æ–∫–µ–Ω ngrok (–µ–≥–æ –º–æ–∂–Ω–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Å https://dashboard.ngrok.com/get-started/your-authtoken):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyngrok'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tempfile\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "# @markdown #..–í—Å—Ç–∞–≤—å—Ç–µ –ø—É—Ç—å –¥–ª—è —Å–≤–æ–∏—Ö –≤–µ—Å–æ–≤ yolo..\n",
        "YOLO_MODEL_PATH = '' # @param {type:\"string\"}\n",
        "# –î–µ—Ñ–æ–ª—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç\n",
        "DEFAULT_CHECK_PROMPT_EN = (\n",
        "    \"Analyze this steel defect. Select the most appropriate class from: \"\n",
        "    \"Crazing, Inclusion, Patches, Pitted Surface, Rolled-in Scale, or Scratches. \"\n",
        "    \"If the class is Scratches, describe its depth and length. \"\n",
        "    \"Respond with only the selected class and a brief justification for the choice.\"\n",
        ")\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π\n",
        "try:\n",
        "    model = YOLO(YOLO_MODEL_PATH)\n",
        "    logger.info(f\"–ú–æ–¥–µ–ª—å YOLO –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {YOLO_MODEL_PATH}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å YOLO: {e}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å: {YOLO_MODEL_PATH}\")\n",
        "    model = None\n",
        "\n",
        "# (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–ª–æ—Ä–µ–Ω—Ü 2)\n",
        "florence_model = None\n",
        "florence_processor = None\n",
        "\n",
        "def load_florence():\n",
        "    global florence_model, florence_processor\n",
        "    if florence_model is None:\n",
        "        try:\n",
        "            logger.info(\"–ó–∞–≥—Ä—É–∑–∫–∞ Florence-2...\")\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "            florence_processor = AutoProcessor.from_pretrained(\n",
        "                \"microsoft/Florence-2-base\",\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            florence_model = AutoModelForCausalLM.from_pretrained(\n",
        "                \"microsoft/Florence-2-base\",\n",
        "                torch_dtype=dtype,\n",
        "                trust_remote_code=True,\n",
        "                attn_implementation=\"sdpa\",\n",
        "                use_flash_attention_2=False\n",
        "            ).to(device)\n",
        "\n",
        "            logger.info(f\"Florence-2 –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device} (—Ä–µ–∂–∏–º SDPA) —Å dtype={dtype}\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Florence-2 —Å SDPA: {e}. –ü—Ä–æ–±—É–µ–º –±–∞–∑–æ–≤—ã–π —Ä–µ–∂–∏–º...\")\n",
        "            try:\n",
        "                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "                dtype = torch.float32\n",
        "                florence_processor = AutoProcessor.from_pretrained(\n",
        "                    \"microsoft/Florence-2-base\",\n",
        "                    trust_remote_code=True\n",
        "                )\n",
        "                florence_model = AutoModelForCausalLM.from_pretrained(\n",
        "                    \"microsoft/Florence-2-base\",\n",
        "                    torch_dtype=dtype,\n",
        "                    trust_remote_code=True,\n",
        "                    _attn_implementation=\"eager\"\n",
        "                ).to(device)\n",
        "                logger.info(f\"Florence-2 –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device} (–±–∞–∑–æ–≤—ã–π —Ä–µ–∂–∏–º 'eager') —Å dtype={dtype}\")\n",
        "            except Exception as e2:\n",
        "                logger.error(f\"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Florence-2: {e2}\")\n",
        "                raise\n",
        "\n",
        "    return florence_model, florence_processor\n",
        "\n",
        "# --- –§—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ---\n",
        "\n",
        "def _apply_enhancements(gray_pil_image):\n",
        "    \"\"\"–ü—Ä–∏–º–µ–Ω—è–µ—Ç CLAHE, —Ñ–∏–ª—å—Ç—Ä—ã –∏ —É–ª—É—á—à–µ–Ω–∏–µ –∫ –ß/–ë PIL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é\"\"\"\n",
        "    img_array = np.array(gray_pil_image)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    enhanced = clahe.apply(img_array)\n",
        "    denoised = cv2.medianBlur(enhanced, 3)\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    morphed = cv2.morphologyEx(denoised, cv2.MORPH_OPEN, kernel, iterations=1)\n",
        "    pil_processed = Image.fromarray(morphed)\n",
        "    sharpened = pil_processed.filter(ImageFilter.SHARPEN)\n",
        "    enhancer_contrast = ImageEnhance.Contrast(sharpened)\n",
        "    contrast_enhanced = enhancer_contrast.enhance(1.2)\n",
        "    enhancer_brightness = ImageEnhance.Brightness(contrast_enhanced)\n",
        "    final_img_pil = enhancer_brightness.enhance(1.1)\n",
        "    return final_img_pil\n",
        "\n",
        "def _base_resize_pad(image, target_size=(640, 640)):\n",
        "    \"\"\"–ë–∞–∑–æ–≤—ã–π —Ä–µ—Å–∞–π–∑ –∏ –ø–∞–¥–¥–∏–Ω–≥, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç PIL Image\"\"\"\n",
        "    if isinstance(image, np.ndarray):\n",
        "        pil_img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) if len(image.shape) == 3 else Image.fromarray(image)\n",
        "    else:\n",
        "        pil_img = image\n",
        "\n",
        "    if pil_img.mode != 'RGB':\n",
        "        pil_img = pil_img.convert('RGB')\n",
        "\n",
        "    pil_img.thumbnail(target_size, Image.Resampling.LANCZOS)\n",
        "    new_img = Image.new(\"RGB\", target_size, (128, 128, 128))\n",
        "    paste_x = (target_size[0] - pil_img.width) // 2\n",
        "    paste_y = (target_size[1] - pil_img.height) // 2\n",
        "    new_img.paste(pil_img, (paste_x, paste_y))\n",
        "    return new_img\n",
        "\n",
        "def remove_shadows_and_highlights(gray_image_np):\n",
        "    \"\"\"–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Ç–µ–Ω–µ–π –∏ –±–ª–∏–∫–æ–≤ (–ß/–ë numpy in -> –ß/–ë numpy out)\"\"\"\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    tophat = cv2.morphologyEx(gray_image_np, cv2.MORPH_TOPHAT, kernel)\n",
        "    blackhat = cv2.morphologyEx(gray_image_np, cv2.MORPH_BLACKHAT, kernel)\n",
        "    result = cv2.add(gray_image_np, tophat)\n",
        "    result = cv2.subtract(result, blackhat)\n",
        "    result = cv2.normalize(result, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    return result\n",
        "\n",
        "def preprocess_image(image, target_size=(640, 640)):\n",
        "    \"\"\"–ë–∞–∑–æ–≤–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (RGB numpy out)\"\"\"\n",
        "    pil_img_padded = _base_resize_pad(image, target_size)\n",
        "    gray_img = pil_img_padded.convert('L')\n",
        "    enhanced_gray_pil = _apply_enhancements(gray_img)\n",
        "    final_rgb = enhanced_gray_pil.convert('RGB')\n",
        "    return np.array(final_rgb)\n",
        "\n",
        "def advanced_preprocess(image, target_size=(640, 640)):\n",
        "    \"\"\"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (RGB numpy out)\"\"\"\n",
        "    pil_img_padded = _base_resize_pad(image, target_size)\n",
        "    gray_img = pil_img_padded.convert('L')\n",
        "    gray_np = np.array(gray_img)\n",
        "    shadows_removed_np = remove_shadows_and_highlights(gray_np)\n",
        "    shadows_removed_pil = Image.fromarray(shadows_removed_np)\n",
        "    enhanced_gray_pil = _apply_enhancements(shadows_removed_pil)\n",
        "    final_rgb = enhanced_gray_pil.convert('RGB')\n",
        "    return np.array(final_rgb)\n",
        "\n",
        "# --- /–§—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ ---\n",
        "\n",
        "\n",
        "def analyze_defect_with_florence(crop_pil, yolo_class_name, custom_prompt=None):\n",
        "    \"\"\"\n",
        "    –ê–Ω–∞–ª–∏–∑ –¥–µ—Ñ–µ–∫—Ç–∞ —Å –ø–æ–º–æ—â—å—é Florence-2\n",
        "    FIX: –ü–æ–ª–Ω—ã–π –æ—Ç–∫–∞–∑ –æ—Ç post_process_generation, —Ä—É—á–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model, processor = load_florence()\n",
        "        if model is None:\n",
        "            return f\"{yolo_class_name} (–û—à–∏–±–∫–∞: Florence-2 –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞)\"\n",
        "\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        if crop_pil.width < 10 or crop_pil.height < 10:\n",
        "            return f\"{yolo_class_name} (crop —Å–ª–∏—à–∫–æ–º –º–∞–ª)\"\n",
        "\n",
        "        # 1. –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç\n",
        "        if custom_prompt and custom_prompt.strip() != \"\":\n",
        "            task_prompt = custom_prompt\n",
        "            logger.info(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç: {custom_prompt}\")\n",
        "        else:\n",
        "            task_prompt = DEFAULT_CHECK_PROMPT_EN\n",
        "            logger.info(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç\")\n",
        "\n",
        "        # 2. –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç\n",
        "        inputs = processor(\n",
        "            text=task_prompt,\n",
        "            images=crop_pil,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        # --- FIX: (TypeError: float vs c10::Half) ---\n",
        "        if device.type == 'cuda' and model.dtype == torch.float16:\n",
        "            if inputs['pixel_values'].dtype == torch.float32:\n",
        "                inputs['pixel_values'] = inputs['pixel_values'].to(torch.float16)\n",
        "        # --- /FIX ---\n",
        "\n",
        "        input_ids_length = inputs['input_ids'].shape[1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                pixel_values=inputs[\"pixel_values\"],\n",
        "                max_new_tokens=150, # –¢–æ–∫–µ–Ω—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –û–¢–í–ï–¢–ê\n",
        "                num_beams=3,\n",
        "                do_sample=False,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        newly_generated_ids = generated_ids[:, input_ids_length:]\n",
        "\n",
        "        # skip_special_tokens=True —É–±–∏—Ä–∞–µ—Ç <eos> –∏ —Ç.–¥.\n",
        "        generated_text = processor.batch_decode(\n",
        "            newly_generated_ids,\n",
        "            skip_special_tokens=True\n",
        "        )[0]\n",
        "\n",
        "        # 7. –û—Ç–∫–∞–∑—ã–≤–∞–µ–º—Å—è –æ—Ç post_process_generation\n",
        "        description = generated_text.strip()\n",
        "        # --- / –ì–õ–ê–í–ù–´–ô FIX ---\n",
        "\n",
        "        if not description: # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø—É—Å—Ç–æ–π\n",
        "            description = yolo_class_name\n",
        "\n",
        "        if yolo_class_name.lower().replace(\"_\", \" \") not in description.lower():\n",
        "             description = f\"{yolo_class_name.title()}: {description}\"\n",
        "\n",
        "        return description\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"–û—à–∏–±–∫–∞ Florence-2 –∞–Ω–∞–ª–∏–∑–∞: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω—É—é –æ—à–∏–±–∫—É –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏\n",
        "        return f\"{yolo_class_name} (–û—à–∏–±–∫–∞: {e})\"\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
        "def detect_image(image, use_advanced=True, use_florence=True,\n",
        "                 custom_prompt=None,\n",
        "                 progress=gr.Progress()):\n",
        "    \"\"\"\n",
        "    –î–µ—Ç–µ–∫—Ü–∏—è –¥–µ—Ñ–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Å –¥–≤—É—Ö–∫–æ–Ω—Ç—É—Ä–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º\n",
        "    \"\"\"\n",
        "    if image is None:\n",
        "        return None, \"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ\", None, None\n",
        "    if model is None:\n",
        "        return None, \"–û—à–∏–±–∫–∞: –ú–æ–¥–µ–ª—å YOLO –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å.\", None, None\n",
        "\n",
        "    progress(0, desc=\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...\")\n",
        "\n",
        "    progress(0.1, desc=\"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...\")\n",
        "    if use_advanced:\n",
        "        preprocessed = advanced_preprocess(image, target_size=(640, 640))\n",
        "    else:\n",
        "        preprocessed = preprocess_image(image, target_size=(640, 640))\n",
        "\n",
        "    progress(0.3, desc=\"–î–µ—Ç–µ–∫—Ü–∏—è YOLOv8...\")\n",
        "    results = model.predict(source=preprocessed, save=False, conf=0.25, verbose=False)\n",
        "    frame_with_boxes = results[0].plot()\n",
        "\n",
        "    preds = []\n",
        "    crops_gallery = []\n",
        "    boxes_list = list(results[0].boxes)\n",
        "\n",
        "    for idx, box in enumerate(boxes_list):\n",
        "        cls_id = int(box.cls[0])\n",
        "        conf = float(box.conf[0])\n",
        "        yolo_label = results[0].names[cls_id]\n",
        "\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
        "        crop_array = preprocessed[y1:y2, x1:x2]\n",
        "\n",
        "        if crop_array.size == 0:\n",
        "            continue\n",
        "\n",
        "        crop_pil = Image.fromarray(crop_array)\n",
        "\n",
        "        final_label = yolo_label\n",
        "        if use_florence and crop_pil.width > 20 and crop_pil.height > 20:\n",
        "            progress(0.3 + 0.7 * (idx / max(1, len(boxes_list))),\n",
        "                     desc=f\"–ê–Ω–∞–ª–∏–∑ –¥–µ—Ñ–µ–∫—Ç–∞ {idx+1}/{len(boxes_list)}...\")\n",
        "\n",
        "            florence_description = analyze_defect_with_florence(\n",
        "                crop_pil,\n",
        "                yolo_label,\n",
        "                custom_prompt\n",
        "            )\n",
        "            final_label = florence_description\n",
        "\n",
        "        preds.append({\n",
        "            \"id\": idx + 1,\n",
        "            \"yolo_class\": yolo_label,\n",
        "            \"florence_description\": final_label if use_florence else None,\n",
        "            \"confidence\": conf,\n",
        "            \"bbox\": [x1, y1, x2, y2]\n",
        "        })\n",
        "\n",
        "        crops_gallery.append((crop_pil, final_label))\n",
        "\n",
        "    progress(1.0, desc=\"–ì–æ—Ç–æ–≤–æ!\")\n",
        "\n",
        "    if preds:\n",
        "        output_text = f\"## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–≤—É—Ö–∫–æ–Ω—Ç—É—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\\n\\n**–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –¥–µ—Ñ–µ–∫—Ç–æ–≤: {len(preds)}**\\n\\n### –î–µ—Ç–∞–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏:\"\n",
        "        for pred in preds:\n",
        "            output_text += f\"\\n\\n**–î–µ—Ñ–µ–∫—Ç #{pred['id']}:**\\n\"\n",
        "            output_text += f\"- YOLO –∫–ª–∞—Å—Å: **{pred['yolo_class']}**\\n\"\n",
        "            output_text += f\"- üîç –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: **{pred['confidence']*100:.1f}%**\\n\"\n",
        "\n",
        "            # --- FIX: –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞ ---\n",
        "            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ, –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å –ò –æ–Ω–æ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–ª–∞—Å—Å–æ–º YOLO\n",
        "            if use_florence and pred['florence_description'] and pred['florence_description'] != pred['yolo_class']:\n",
        "                desc = pred['florence_description']\n",
        "\n",
        "                # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å \"–ö–ª–∞—Å—Å: \", –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å\n",
        "                if desc.startswith(f\"{pred['yolo_class'].title()}: \"):\n",
        "                    desc = desc[len(pred['yolo_class'])+2:].strip()\n",
        "\n",
        "                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—á–∏—â–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (–∏–ª–∏ –æ—à–∏–±–∫—É)\n",
        "                output_text += f\"- –û—Ç–≤–µ—Ç Florence-2: _{desc}_\"\n",
        "            # --- /FIX ---\n",
        "\n",
        "            output_text += f\"\\n- –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox: [{pred['bbox'][0]}, {pred['bbox'][1]}, {pred['bbox'][2]}, {pred['bbox'][3]}]\"\n",
        "    else:\n",
        "        output_text = \"**–î–µ—Ñ–µ–∫—Ç–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ**\"\n",
        "\n",
        "    output_text += f\"\"\"\n",
        "\n",
        "---\n",
        "- ‚úÖ –†–∞–∑–º–µ—Ä: 640x640\n",
        "- ‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞: {'–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è (—Ç–µ–Ω–∏/–±–ª–∏–∫–∏ —É–¥–∞–ª–µ–Ω—ã)' if use_advanced else '–ë–∞–∑–æ–≤–∞—è'}\n",
        "- ‚úÖ –ö–æ–Ω—Ç—É—Ä 1 (YOLO): –î–µ—Ç–µ–∫—Ü–∏—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ ‚úì\n",
        "- {'‚úÖ' if use_florence else '‚ùå'} –ö–æ–Ω—Ç—É—Ä 2 (Florence-2): {'–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ ‚úì' if use_florence else '–û—Ç–∫–ª—é—á–µ–Ω'}\n",
        "\"\"\"\n",
        "\n",
        "    return frame_with_boxes, output_text, preprocessed, crops_gallery if crops_gallery else None\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ\n",
        "def detect_video(video, use_advanced=False, use_florence=False,\n",
        "                 custom_prompt=None,\n",
        "                 progress=gr.Progress()):\n",
        "    if video is None:\n",
        "        return None, \"–í–∏–¥–µ–æ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ\", None\n",
        "    if model is None:\n",
        "        return None, \"–û—à–∏–±–∫–∞: –ú–æ–¥–µ–ª—å YOLO –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.\", None\n",
        "\n",
        "    temp_out = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\")\n",
        "    cap = cv2.VideoCapture(video)\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 20\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total_frames == 0:\n",
        "        return None, \"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –≤–∏–¥–µ–æ\", None\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = None\n",
        "\n",
        "    frame_count = 0\n",
        "    total_detections = 0\n",
        "    defect_counts = {}\n",
        "    first_frame_processed = None\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "        progress(frame_count / total_frames, desc=f\"–ö–∞–¥—Ä {frame_count}/{total_frames}\")\n",
        "\n",
        "        if use_advanced:\n",
        "            processed_frame = advanced_preprocess(frame, target_size=(640, 640))\n",
        "        else:\n",
        "            processed_frame = preprocess_image(frame, target_size=(640, 640))\n",
        "\n",
        "        if first_frame_processed is None:\n",
        "            first_frame_processed = processed_frame.copy()\n",
        "\n",
        "        results = model.predict(processed_frame, save=False, conf=0.25, verbose=False)\n",
        "        frame_out = results[0].plot()\n",
        "\n",
        "        for box in results[0].boxes:\n",
        "            total_detections += 1\n",
        "            cls_id = int(box.cls[0])\n",
        "            label = results[0].names[cls_id]\n",
        "\n",
        "            if use_florence: # ‚ö†Ô∏è –≠—Ç–æ –≤—Å–µ –µ—â–µ –û–ß–ï–ù–¨ –º–µ–¥–ª–µ–Ω–Ω–æ\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
        "                crop = processed_frame[y1:y2, x1:x2]\n",
        "                if crop.size > 0:\n",
        "                    crop_pil = Image.fromarray(crop)\n",
        "                    florence_desc = analyze_defect_with_florence(\n",
        "                        crop_pil, label, custom_prompt\n",
        "                    )\n",
        "                    label = florence_desc\n",
        "\n",
        "            defect_counts[label] = defect_counts.get(label, 0) + 1\n",
        "\n",
        "        if out is None:\n",
        "            h, w, _ = frame_out.shape\n",
        "            out = cv2.VideoWriter(temp_out.name, fourcc, fps, (w, h))\n",
        "\n",
        "        out.write(frame_out)\n",
        "\n",
        "    cap.release()\n",
        "    if out: out.release()\n",
        "\n",
        "    stats_text = f\"## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ\\n\\n### –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:\\n- üé¨ **–í—Å–µ–≥–æ –∫–∞–¥—Ä–æ–≤:** {total_frames}\\n- üîç **–í—Å–µ–≥–æ –¥–µ—Ç–µ–∫—Ü–∏–π:** {total_detections}\\n- üìà **–°—Ä–µ–¥–Ω–µ–µ –Ω–∞ –∫–∞–¥—Ä:** {total_detections/max(total_frames, 1):.2f}\\n\\n### –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–µ—Ñ–µ–∫—Ç–æ–≤:\"\n",
        "\n",
        "    if defect_counts and total_detections > 0:\n",
        "        for defect, count in sorted(defect_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "            percentage = (count / total_detections) * 100\n",
        "            stats_text += f\"\\n- **{defect}:** {count} ({percentage:.1f}%)\"\n",
        "    else:\n",
        "        stats_text += \"\\n–î–µ—Ñ–µ–∫—Ç–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ\"\n",
        "\n",
        "    stats_text += f\"\\n\\n---\\n### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏:\\n- ‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞: {'–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è' if use_advanced else '–ë–∞–∑–æ–≤–∞—è'}\\n- {'‚úÖ' if use_florence else '‚ùå'} Florence-2: {'–í–∫–ª—é—á–µ–Ω (–º–µ–¥–ª–µ–Ω–Ω–µ–µ)' if use_florence else '–û—Ç–∫–ª—é—á–µ–Ω'}\"\n",
        "\n",
        "    return temp_out.name, stats_text, first_frame_processed\n",
        "\n",
        "# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio\n",
        "with gr.Blocks(title=\"YOLOv8 + Florence-2 Steel Defect Detection\", theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    gr.Markdown(\"# üîç YOLOv8 + Florence-2: –î–≤—É—Ö–∫–æ–Ω—Ç—É—Ä–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–µ—Ñ–µ–∫—Ç–æ—Å–∫–æ–ø–∏–∏\\n## NEU-DET: –î–µ—Ç–µ–∫—Ü–∏—è –¥–µ—Ñ–µ–∫—Ç–æ–≤ —Å—Ç–∞–ª—å–Ω—ã—Ö –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π\")\n",
        "\n",
        "    with gr.Tabs() as tabs:\n",
        "        # –í–∫–ª–∞–¥–∫–∞ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
        "        with gr.Tab(\"üì∏ –†–µ–∂–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\", id=0):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    image_input = gr.Image(label=\"–í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\", type=\"numpy\")\n",
        "\n",
        "                    with gr.Accordion(\"–ù–∞—Å—Ç—Ä–æ–π–∫–∏\", open=True):\n",
        "                        use_advanced_img = gr.Checkbox(\n",
        "                            value=True,\n",
        "                            label=\"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞\",\n",
        "                            info=\"–£–¥–∞–ª–µ–Ω–∏–µ —Ç–µ–Ω–µ–π –∏ –±–ª–∏–∫–æ–≤ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)\"\n",
        "                        )\n",
        "                        use_florence_img = gr.Checkbox(\n",
        "                            value=True,\n",
        "                            label=\"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Florence-2\",\n",
        "                            info=\"–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ñ–µ–∫—Ç–∞\"\n",
        "                        )\n",
        "\n",
        "                        custom_prompt_img = gr.Textbox(\n",
        "                            label=\"–ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\",\n",
        "                            placeholder=f\"–ï—Å–ª–∏ –ø—É—Å—Ç–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π: '{DEFAULT_CHECK_PROMPT_EN[:50]}...'\",\n",
        "                            lines=3\n",
        "                        )\n",
        "\n",
        "                    image_button = gr.Button(\"–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    image_output = gr.Image(label=\"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏\", type=\"numpy\")\n",
        "                    image_stats = gr.Markdown(\"### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—è–≤—è—Ç—Å—è –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    image_preprocessed = gr.Image(\n",
        "                        label=\"–ü–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\",\n",
        "                        # FIX: –£–¥–∞–ª–µ–Ω 'info'\n",
        "                        type=\"numpy\"\n",
        "                    )\n",
        "\n",
        "                with gr.Column():\n",
        "                    image_crops = gr.Gallery(\n",
        "                        label=\"–í—ã—Ä–µ–∑–∞–Ω–Ω—ã–µ –¥–µ—Ñ–µ–∫—Ç—ã\",\n",
        "                        columns=3,\n",
        "                        rows=2,\n",
        "                        object_fit=\"contain\"\n",
        "                    )\n",
        "\n",
        "            image_button.click(\n",
        "                fn=detect_image,\n",
        "                inputs=[image_input, use_advanced_img, use_florence_img, custom_prompt_img],\n",
        "                outputs=[image_output, image_stats, image_preprocessed, image_crops]\n",
        "            )\n",
        "\n",
        "        # –í–∫–ª–∞–¥–∫–∞ –¥–ª—è –≤–∏–¥–µ–æ\n",
        "        with gr.Tab(\"üé¨ –†–µ–∂–∏–º –≤–∏–¥–µ–æ\", id=1):\n",
        "            gr.Markdown(\"### –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ\\n‚ö†Ô∏è Florence-2 –¥–ª—è –≤–∏–¥–µ–æ –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω—ã–π! –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ—Ç–∫–ª—é—á–∏—Ç—å.\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    video_input = gr.Video(label=\"–í—Ö–æ–¥–Ω–æ–µ –≤–∏–¥–µ–æ\")\n",
        "\n",
        "                    with gr.Accordion(\"–ù–∞—Å—Ç—Ä–æ–π–∫–∏\", open=True):\n",
        "                        use_advanced_vid = gr.Checkbox(value=False, label=\"üîß –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞\", info=\"–ú–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ\")\n",
        "                        use_florence_vid = gr.Checkbox(value=False, label=\"ü§ñ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Florence-2\", info=\"‚ö†Ô∏è –û–ß–ï–ù–¨ –º–µ–¥–ª–µ–Ω–Ω–æ! ~1 —Å–µ–∫ –Ω–∞ –¥–µ—Ñ–µ–∫—Ç\")\n",
        "                        custom_prompt_vid = gr.Textbox(\n",
        "                            label=\"–ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\",\n",
        "                            placeholder=f\"–ï—Å–ª–∏ –ø—É—Å—Ç–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π: '{DEFAULT_CHECK_PROMPT_EN[:50]}...'\",\n",
        "                            lines=3\n",
        "                        )\n",
        "\n",
        "                    video_button = gr.Button(\"–û–±—Ä–∞–±–æ—Ç–∞—Ç—å\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    video_output = gr.Video(label=\"–†–µ–∑—É–ª—å—Ç–∞—Ç\")\n",
        "                    video_stats = gr.Markdown(\"### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\")\n",
        "\n",
        "            with gr.Row():\n",
        "                video_preview = gr.Image(label=\"–ü–µ—Ä–≤—ã–π –∫–∞–¥—Ä (–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞)\", type=\"numpy\")\n",
        "\n",
        "            video_button.click(\n",
        "                fn=detect_video,\n",
        "                inputs=[video_input, use_advanced_vid, use_florence_vid, custom_prompt_vid],\n",
        "                outputs=[video_output, video_stats, video_preview]\n",
        "            )\n",
        "\n",
        "    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ\n",
        "    with gr.Accordion(\"–û –¥–≤—É—Ö–∫–æ–Ω—Ç—É—Ä–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ\", open=False):\n",
        "        gr.Markdown(f\"\"\"\n",
        "        ### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
        "        **–ö–æ–Ω—Ç—É—Ä 1: YOLOv8 (NEU-DET)**\n",
        "        - –ó–∞–¥–∞—á–∞: –õ–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è (–≥–¥–µ –¥–µ—Ñ–µ–∫—Ç?)\n",
        "\n",
        "        **–ö–æ–Ω—Ç—É—Ä 2: Florence-2-base**\n",
        "        - –ó–∞–¥–∞—á–∞: –ü–æ–Ω–∏–º–∞–Ω–∏–µ (—á—Ç–æ –∑–∞ –¥–µ—Ñ–µ–∫—Ç?)\n",
        "\n",
        "        ### –ö–ª–∞—Å—Å—ã NEU-DET:\n",
        "        1. Crazing (Cr), 2. Inclusion (In), 3. Patches (Pa),\n",
        "        4. Pitted Surface (PS), 5. Rolled-in Scale (RS), 6. Scratches (Sc)\n",
        "        \"\"\")\n",
        "\n",
        "    with gr.Accordion(\"–°–æ–≤–µ—Ç—ã –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é\", open=False):\n",
        "        gr.Markdown(f\"\"\"\n",
        "        ### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n",
        "\n",
        "        **–î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:**\n",
        "        - ‚úÖ –í–∫–ª—é—á–∏—Ç–µ Florence-2 –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n",
        "        - ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–∞—Å—Ç–æ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á\n",
        "\n",
        "        **–î–ª—è –≤–∏–¥–µ–æ:**\n",
        "        - ‚ö†Ô∏è –û—Ç–∫–ª—é—á–∏—Ç–µ Florence-2 (–æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ)\n",
        "\n",
        "        **–ü—Ä–æ–º–ø—Ç—ã:**\n",
        "        - –û—Å—Ç–∞–≤—å—Ç–µ –ø–æ–ª–µ –ø—É—Å—Ç—ã–º –¥–ª—è –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è + –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ).\n",
        "        - **–î–µ—Ñ–æ–ª—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏):** `{DEFAULT_CHECK_PROMPT_EN}`\n",
        "        - **–ü—Ä–∏–º–µ—Ä –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ:** \"–û–ø–∏—à–∏ —Ç–∏–ø, —Ü–≤–µ—Ç –∏ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å –¥–µ—Ñ–µ–∫—Ç–∞\"\n",
        "        \"\"\")\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫\n",
        "if __name__ == \"__main__\":\n",
        "    if model is None:\n",
        "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        print(\"!!! –í–ù–ò–ú–ê–ù–ò–ï: –ú–æ–¥–µ–ª—å YOLO –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.                 !!!\")\n",
        "        print(f\"!!! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π YOLO_MODEL_PATH: {YOLO_MODEL_PATH} !!!\")\n",
        "        print(\"!!! –î–µ–º–æ –∑–∞–ø—É—Å—Ç–∏—Ç—Å—è, –Ω–æ –Ω–µ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å.           !!!\")\n",
        "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "\n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º Florence-2 –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ, —á—Ç–æ–±—ã –ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –±—ã–ª –±—ã—Å—Ç—Ä–µ–µ\n",
        "    logger.info(\"–ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ Florence-2...\")\n",
        "    load_florence()\n",
        "\n",
        "    demo.launch(share=True, debug=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "vkRxkm46v41_",
        "outputId": "1f873f76-03b5-42e7-adfb-b2fc9cad1aea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2253580511.py, line 14)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2253580511.py\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    YOLO_MODEL_PATH = @param {type:\"string\"}\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏\n",
        "def run_example(task_prompt, image, text_input=None):\n",
        "    \"\"\"\n",
        "    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º.\n",
        "    \"\"\"\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û–±—ä–µ–¥–∏–Ω—è–µ–º task_prompt –∏ text_input –≤ –æ–¥–∏–Ω –∞—Ä–≥—É–º–µ–Ω—Ç 'text' ---\n",
        "\n",
        "    # 1. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –∑–∞–¥–∞—á–∏ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞\n",
        "    full_prompt = task_prompt\n",
        "    if text_input:\n",
        "        full_prompt += text_input\n",
        "    else:\n",
        "        full_prompt = f\"{task_prompt}\\nCaption:\"\n",
        "    # 2. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ (–ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç 'full_prompt')\n",
        "    inputs = processor(\n",
        "        text=full_prompt,\n",
        "        images=image,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    # -----------------------------------------------------------------------------\n",
        "\n",
        "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=processor.tokenizer.eos_token_id,\n",
        "        do_sample=False,\n",
        "        num_beams=3 # –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
        "    )\n",
        "\n",
        "    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "\n",
        "    # –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞\n",
        "    parsed_text = processor.post_process_generation(\n",
        "        generated_text,\n",
        "        task_prompt=task_prompt,\n",
        "        image_size=(image.width, image.height)\n",
        "    )\n",
        "    return parsed_text"
      ],
      "metadata": {
        "id": "oah2YOXJjz0O"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# 1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_DTYPE = torch.bfloat16\n",
        "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
        "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö (DType): {MODEL_DTYPE}\")\n",
        "\n",
        "# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞\n",
        "model_id = \"microsoft/Florence-2-base\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=MODEL_DTYPE,\n",
        "    trust_remote_code=True\n",
        ").to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
        "print(\"–ú–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä Florence-2 —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.\")\n",
        "\n",
        "# 3. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏\n",
        "# 3. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏\n",
        "def run_example(task_prompt, image, text_input=None, desired_new=48):\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    # 1. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ (–ò–°–ü–†–ê–í–õ–ï–ù–û!)\n",
        "    # –î–ª—è Florence-2 –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω,\n",
        "    # –∏ —Ç–æ–ª—å–∫–æ –¥–ª—è VQA –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç Q/A\n",
        "\n",
        "    if task_prompt.strip().upper() == \"<VQA>\" and text_input:\n",
        "        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç VQA\n",
        "        full_prompt = f\"{task_prompt}\\nQ: {text_input}\\nA:\"\n",
        "        # –ü—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–≤ —Å–ª—É—á–∞–µ VQA)\n",
        "        generation_prefix = \"A:\"\n",
        "    else:\n",
        "        # –î–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á (CAPTION, OD –∏ —Ç.–¥.)\n",
        "        # –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ–ª–∂–µ–Ω –≤–∏–¥–µ—Ç—å —Ç–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω –∑–∞–¥–∞—á–∏.\n",
        "        full_prompt = task_prompt.strip()\n",
        "        generation_prefix = \"\" # –ù–µ –∏—â–µ–º –ø—Ä–µ—Ñ–∏–∫—Å\n",
        "\n",
        "    # 2. –§–æ—Ä–º–∏—Ä—É–µ–º inputs\n",
        "    inputs = processor(text=full_prompt, images=image, return_tensors=\"pt\")\n",
        "\n",
        "    # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
        "    for k, v in inputs.items():\n",
        "        if torch.is_floating_point(v):\n",
        "            inputs[k] = v.to(MODEL_DTYPE).to(device)\n",
        "        else:\n",
        "            inputs[k] = v.to(device)\n",
        "\n",
        "    # ... (–û—Å—Ç–∞–≤–ª—è–µ–º gen_kwargs –∏ max_length –∫–∞–∫ –µ—Å—Ç—å)\n",
        "    gen_kwargs = {\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        \"pixel_values\": inputs[\"pixel_values\"],\n",
        "        \"attention_mask\": inputs[\"attention_mask\"],\n",
        "    }\n",
        "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "    max_length = prompt_len + desired_new\n",
        "    pad_id = getattr(processor.tokenizer, \"pad_token_id\", None) or processor.tokenizer.eos_token_id\n",
        "    eos_id = processor.tokenizer.eos_token_id\n",
        "\n",
        "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
        "    generated = model.generate(\n",
        "        **gen_kwargs,\n",
        "        max_length=max_length,\n",
        "        do_sample=False,\n",
        "        num_beams=3,\n",
        "        pad_token_id=pad_id,\n",
        "        eos_token_id=eos_id,\n",
        "        early_stopping=True,\n",
        "        return_dict_in_generate=True\n",
        "    )\n",
        "\n",
        "    seq = generated.sequences[0]\n",
        "    decoded = processor.tokenizer.decode(seq, skip_special_tokens=True)\n",
        "\n",
        "    # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ (–Ω–∞ –æ—Å–Ω–æ–≤–µ VQA-–ø—Ä–µ—Ñ–∏–∫—Å–∞)\n",
        "    if generation_prefix and generation_prefix in decoded:\n",
        "        # –ï—Å–ª–∏ —ç—Ç–æ VQA, —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ \"A:\"\n",
        "        parsed_text = decoded.split(generation_prefix, 1)[-1].strip()\n",
        "    else:\n",
        "        # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á (CAPTION) - –ø—Ä–æ—Å—Ç–æ —É–¥–∞–ª—è–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –Ω–∞—á–∞–ª–∞\n",
        "        parsed_text = decoded[len(full_prompt):].strip() if decoded.startswith(full_prompt) else decoded.strip()\n",
        "        # –î–ª—è Captioning –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ —Ç–æ, —á—Ç–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∞\n",
        "        if task_prompt.strip().upper() == \"<CAPTION>\":\n",
        "             # –ß–∞—Å—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞—á–∏–Ω–∞–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∞\n",
        "             # –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (—Ç–∞–∫ –∫–∞–∫ skip_special_tokens=True)\n",
        "             parsed_text = decoded.strip()\n",
        "\n",
        "    return parsed_text\n",
        "\n",
        "# 4. –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "image_url = \"https://thumb.cloud.mail.ru/weblink/thumb/xw1/uqhp/7mBK8QqGo/IMG_6873.JPG\"\n",
        "response = requests.get(image_url)\n",
        "image = Image.open(BytesIO(response.content))\n",
        "print(\"\\n–¢–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ.\")\n",
        "\n",
        "prompt_vqa = \"<DETAILED_CAPTION>\"\n",
        "#question = \"Where is the cat?\"\n",
        "result_vqa = run_example(prompt_vqa, image, text_input=None, desired_new=48)\n",
        "\n",
        "print(\"\\n--- –†–µ–∑—É–ª—å—Ç–∞—Ç VQA ---\")\n",
        "print(f\"–û—Ç–≤–µ—Ç Florence-2: {result_vqa}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVdfFRfShPRP",
        "outputId": "579d1cec-4ffd-4a06-a34a-405621626ed2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n",
            "–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö (DType): torch.bfloat16\n",
            "–ú–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä Florence-2 —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.\n",
            "\n",
            "–¢–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ.\n",
            "\n",
            "--- –†–µ–∑—É–ª—å—Ç–∞—Ç VQA ---\n",
            "–û—Ç–≤–µ—Ç Florence-2: The image shows a man standing on top of a stage, holding a microphone in his hand. Behind him is a wall with a curtain draped over it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_MJ4dmtljzvE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}